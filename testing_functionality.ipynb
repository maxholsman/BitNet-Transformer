{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD\n",
    "def absmax_quantization(x, bit=8):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    # find the maximum absolute value in the tensor\n",
    "    max_val = torch.max(torch.abs(x))\n",
    "    \n",
    "    # using the max values, we can calculate the scaling factor for each value in the tensor to map it to the range appropriate range\n",
    "    scale_factor = Qb / max_val\n",
    "    \n",
    "    # now we can quantize the tensor, rounding to the nearest integer\n",
    "    x = torch.round(x * scale_factor)\n",
    "    \n",
    "    return x.to(torch.int8), max_val\n",
    "\n",
    "def absmax_dequantization(x, max_val, bit=8):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    reverse_scale_factor = max_val / Qb\n",
    "    \n",
    "    x = x * reverse_scale_factor\n",
    "    \n",
    "    return x.to(torch.float32) # return to float32 which is original precision\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, groups=1, bit=8, nl_next=False, bias=True):\n",
    "        super(BitLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.groups = groups\n",
    "        # print(f\"input: {in_features}, output: {out_features}, groups: {groups}\")\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.Tensor(self.out_features, self.in_features))\n",
    "        \n",
    "        # print(f\"weights: {self.weights.shape}\")\n",
    "        # Upon initialization, the weights will be randomly initialized using the kaiming uniform method\n",
    "        self.parameter_initialization()\n",
    "        \n",
    "    def parameter_initialization(self):\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = self.weights.view(self.groups, -1, self.in_features)\n",
    "        \n",
    "        # normalize to zero mean\n",
    "        weights = weights - weights.mean(dim=[1, 2], keepdim=True)\n",
    "        \n",
    "        # quantize weights\n",
    "        weights = torch.sign(weights)\n",
    "        \n",
    "        # calculate beta as 1-norm of weights divided by n*m\n",
    "        beta = (torch.norm(weights, p=1, dim=[1, 2], keepdim=True) / \n",
    "                              (weights.shape[1] * weights.shape[2]))\n",
    "        \n",
    "        #scale the weights by beta\n",
    "        weights = weights * beta\n",
    "        \n",
    "        #reshape to original shape\n",
    "        weights = weights.view(self.out_features, self.in_features)\n",
    "        \n",
    "        # get quantized inputs\n",
    "        quantized_input, gamma = absmax_quantization(x)\n",
    "        \n",
    "        # forward pass\n",
    "        # print(f\"weights: {weights}\")\n",
    "        # print(f\"quantized input: {quantized_input}\")\n",
    "        output = torch.matmul(quantized_input.float(), weights.t())\n",
    "        \n",
    "        # print(f\"output: {output}\")\n",
    "        output = absmax_dequantization(output, gamma)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantization(x, bit=8, nl_next=False):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    # find the maximum absolute value in the tensor\n",
    "    max_val = torch.max(torch.abs(x))\n",
    "    min_val = torch.min(x)\n",
    "    \n",
    "    if nl_next:\n",
    "        shifted_x = x - min_val\n",
    "        max_val = torch.max(torch.abs(shifted_x))\n",
    "        \n",
    "        scale_factor = Qb / max_val\n",
    "        x = torch.round(shifted_x * scale_factor)\n",
    "    else:\n",
    "        # using the max values, we can calculate the scaling factor for each value in the tensor to map it to the range appropriate range\n",
    "        scale_factor = Qb / max_val\n",
    "        \n",
    "        # now we can quantize the tensor, rounding to the nearest integer\n",
    "        x = torch.round(x * scale_factor)\n",
    "    \n",
    "    return x.to(torch.int8), max_val, min_val\n",
    "\n",
    "def absmax_dequantization(x, max_val, nl_next=False, min_val=None, bit=8):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    reverse_scale_factor = max_val / Qb\n",
    "    \n",
    "    x = x * reverse_scale_factor\n",
    "    \n",
    "    return x.to(torch.float32) # return to float32 which is original precision\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, groups=1, bit=8, nl_next=False, bias=True):\n",
    "        super(BitLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.groups = groups\n",
    "        # print(f\"input: {in_features}, output: {out_features}, groups: {groups}\")\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.Tensor(self.out_features, self.in_features))\n",
    "        \n",
    "        # print(f\"weights: {self.weights.shape}\")\n",
    "        # Upon initialization, the weights will be randomly initialized using the kaiming uniform method\n",
    "        self.parameter_initialization()\n",
    "        \n",
    "    def parameter_initialization(self):\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = self.weights.view(self.groups, -1, self.in_features)\n",
    "        \n",
    "        # normalize to zero mean\n",
    "        weights = weights - weights.mean(dim=[1, 2], keepdim=True)\n",
    "        \n",
    "        # quantize weights\n",
    "        weights = torch.sign(weights)\n",
    "        \n",
    "        # calculate beta as 1-norm of weights divided by n*m\n",
    "        beta = (torch.norm(weights, p=1, dim=[1, 2], keepdim=True) / \n",
    "                              (weights.shape[1] * weights.shape[2]))\n",
    "        \n",
    "        #scale the weights by beta\n",
    "        weights = weights * beta\n",
    "        \n",
    "        #reshape to original shape\n",
    "        weights = weights.view(self.out_features, self.in_features)\n",
    "        \n",
    "        # get quantized inputs\n",
    "        quantized_input, gamma = absmax_quantization(x)\n",
    "        \n",
    "        # forward pass\n",
    "        # print(f\"weights: {weights}\")\n",
    "        # print(f\"quantized input: {quantized_input}\")\n",
    "        output = torch.matmul(quantized_input.float(), weights.t())\n",
    "        \n",
    "        # print(f\"output: {output}\")\n",
    "        output = absmax_dequantization(output, gamma)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-394.6103,  970.2345, -608.3282, -295.1694, -837.0201,  673.8638,\n",
      "           42.3396,  -14.7834, -921.3655, -161.1737, -599.8037,  768.5757,\n",
      "         -322.7930,  949.8514,  105.5886, -375.3987]])\n",
      "output: tensor([[ -52,  127,  -80,  -39, -110,   89,    6,   -2, -122,  -21,  -79,  101,\n",
      "          -43,  125,   14,  -50]], dtype=torch.int8)\n",
      "recon_input: tensor([[-394.1578,  962.6545, -606.3965, -295.6183, -833.7953,  674.6161,\n",
      "           45.4797,  -15.1599, -924.7548, -159.1791, -598.8166,  765.5757,\n",
      "         -325.9381,  947.4946,  106.1194, -378.9979]])\n",
      "output: tensor([[ 36, 127,  21,  42,   6, 108,  65,  61,   0,  51,  22, 114,  41, 127,\n",
      "          69,  37]], dtype=torch.int8)\n",
      "recon input: tensor([[ 532.0125, 1876.8219,  310.3406,  620.6812,   88.6687, 1596.0375,\n",
      "          960.5781,  901.4656,    0.0000,  753.6844,  325.1187, 1684.7062,\n",
      "          605.9031, 1876.8219, 1019.6906,  546.7906]])\n",
      "recon input: tensor([[-389.3530,  955.4564, -611.0249, -300.6843, -832.6967,  674.6720,\n",
      "           39.2126,  -19.8998, -921.3655, -167.6811, -596.2467,  763.3407,\n",
      "         -315.4623,  955.4564,   98.3251, -374.5749]])\n"
     ]
    }
   ],
   "source": [
    "# make a test input of values ranging from -1000 to 1000\n",
    "input = torch.rand(1, 16) * 2000 - 1000\n",
    "print(f\"x: {input}\")\n",
    "output, max_val, min_val = absmax_quantization(input)\n",
    "print(f\"output: {output}\")\n",
    "recon_input = absmax_dequantization(output, max_val)\n",
    "print(f\"recon_input: {recon_input}\")\n",
    "output, max_val, min_val = absmax_quantization(input, nl_next=True)\n",
    "print(f\"output: {output}\")\n",
    "recon_input = absmax_dequantization(output, max_val)\n",
    "print(f\"recon input: {recon_input}\")\n",
    "recon_input = recon_input + min_val\n",
    "print(f\"recon input: {recon_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
