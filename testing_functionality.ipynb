{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD\n",
    "def absmax_quantization(x, bit=8):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    # find the maximum absolute value in the tensor\n",
    "    max_val = torch.max(torch.abs(x))\n",
    "    \n",
    "    # using the max values, we can calculate the scaling factor for each value in the tensor to map it to the range appropriate range\n",
    "    scale_factor = Qb / max_val\n",
    "    \n",
    "    # now we can quantize the tensor, rounding to the nearest integer\n",
    "    x = torch.round(x * scale_factor)\n",
    "    \n",
    "    return x.to(torch.int8), max_val\n",
    "\n",
    "def absmax_dequantization(x, max_val, bit=8):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    reverse_scale_factor = max_val / Qb\n",
    "    \n",
    "    x = x * reverse_scale_factor\n",
    "    \n",
    "    return x.to(torch.float32) # return to float32 which is original precision\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, groups=1, bit=8, nl_next=False, bias=True):\n",
    "        super(BitLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.groups = groups\n",
    "        # print(f\"input: {in_features}, output: {out_features}, groups: {groups}\")\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.Tensor(self.out_features, self.in_features))\n",
    "        \n",
    "        # print(f\"weights: {self.weights.shape}\")\n",
    "        # Upon initialization, the weights will be randomly initialized using the kaiming uniform method\n",
    "        self.parameter_initialization()\n",
    "        \n",
    "    def parameter_initialization(self):\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights = self.weights.view(self.groups, -1, self.in_features)\n",
    "        \n",
    "        # normalize to zero mean\n",
    "        weights = weights - weights.mean(dim=[1, 2], keepdim=True)\n",
    "        \n",
    "        # quantize weights\n",
    "        weights = torch.sign(weights)\n",
    "        \n",
    "        # calculate beta as 1-norm of weights divided by n*m\n",
    "        beta = (torch.norm(weights, p=1, dim=[1, 2], keepdim=True) / \n",
    "                              (weights.shape[1] * weights.shape[2]))\n",
    "        \n",
    "        #scale the weights by beta\n",
    "        weights = weights * beta\n",
    "        \n",
    "        #reshape to original shape\n",
    "        weights = weights.view(self.out_features, self.in_features)\n",
    "        \n",
    "        # get quantized inputs\n",
    "        quantized_input, gamma = absmax_quantization(x)\n",
    "        \n",
    "        # forward pass\n",
    "        # print(f\"weights: {weights}\")\n",
    "        # print(f\"quantized input: {quantized_input}\")\n",
    "        output = torch.matmul(quantized_input.float(), weights.t())\n",
    "        \n",
    "        # print(f\"output: {output}\")\n",
    "        output = absmax_dequantization(output, gamma)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantization(x, bit=8, nl_next=False):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    # find the maximum absolute value in the tensor\n",
    "    max_val = torch.max(torch.abs(x))\n",
    "    min_val = torch.min(x)\n",
    "    \n",
    "    print(f\"data type before quantization: {x.type()}\")\n",
    "    \n",
    "    if nl_next:\n",
    "        shifted_x = x - min_val\n",
    "        max_val = torch.max(torch.abs(shifted_x))\n",
    "        \n",
    "        scale_factor = Qb / max_val\n",
    "        x = torch.round(shifted_x * scale_factor)\n",
    "    else:\n",
    "        # using the max values, we can calculate the scaling factor for each value in the tensor to map it to the range appropriate range\n",
    "        scale_factor = Qb / max_val\n",
    "        \n",
    "        # now we can quantize the tensor, rounding to the nearest integer\n",
    "        x = torch.round(x * scale_factor)\n",
    "    \n",
    "    dequant = max_val / Qb\n",
    "    \n",
    "    return x.to(torch.int8), dequant, max_val, min_val\n",
    "\n",
    "def absmax_dequantization(x, max_val, nl_next=False, min_val=None, bit=8):\n",
    "    Qb = 2**(bit - 1)\n",
    "    \n",
    "    reverse_scale_factor = max_val / Qb\n",
    "    \n",
    "    x = x * reverse_scale_factor\n",
    "    \n",
    "    return x.to(torch.float32) # return to float32 which is original precision\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, groups=1, bit=8, nl_next=False, bias=True):\n",
    "        super(BitLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.groups = groups\n",
    "        self.nl_next = nl_next\n",
    "        \n",
    "        if bias is True:\n",
    "            self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        \n",
    "        # # print(f\"weights: {self.weights.shape}\")\n",
    "        # # Upon initialization, the weights will be randomly initialized using the kaiming uniform method\n",
    "        # self.parameter_initialization()\n",
    "        \n",
    "    # def parameter_initialization(self):\n",
    "    #     nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        input_norm = F.layer_norm(x, (self.in_features,))\n",
    "        \n",
    "        input_quant, dequant, gamma, eta = absmax_quantization(input_norm, nl_next=self.nl_next)\n",
    "        \n",
    "        print(f\"data type after quantization: {input_quant.type()}\")\n",
    "        \n",
    "        weight_quant = torch.sign(self.weights)\n",
    "        \n",
    "        print(f\"weight quant: {weight_quant}\")\n",
    "        \n",
    "        output = torch.matmul(input_quant.float(), weight_quant.t())\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.unsqueeze(0).expand_as(output)\n",
    "            \n",
    "        beta = torch.norm(self.weights, p=1) / (self.in_features * self.out_features)\n",
    "        \n",
    "        output = output * dequant * beta\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 953.1671,  -93.2529, -873.9128,  526.3042,  212.9384,  716.6527,\n",
      "          397.1938, -655.5541, -177.6257,  196.9402,  722.6052,   18.9512,\n",
      "         -131.3445,  375.2721,  221.9041, -706.2743]])\n",
      "data type before quantization: torch.FloatTensor\n",
      "output: tensor([[ 127,  -13, -117,   71,   29,   96,   53,  -88,  -24,   26,   97,    3,\n",
      "          -18,   50,   30,  -95]], dtype=torch.int8), torch.CharTensor\n",
      "recon_input: tensor([[ 945.7205,  -96.8060, -871.2543,  528.7099,  215.9519,  714.8754,\n",
      "          394.6707, -655.3024, -178.7188,  193.6121,  722.3220,   22.3399,\n",
      "         -134.0391,  372.3309,  223.3985, -707.4287]]), torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# make a test input of values ranging from -1000 to 1000\n",
    "input = torch.rand(1, 16) * 2000 - 1000\n",
    "print(f\"x: {input}\")\n",
    "output, dequant, max_val, min_val = absmax_quantization(input)\n",
    "print(f\"output: {output}, {output.type()}\")\n",
    "recon_input = dequant * output\n",
    "print(f\"recon_input: {recon_input}, {recon_input.type()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type before quantization: torch.FloatTensor\n",
      "data type after quantization: torch.CharTensor\n",
      "weight quant: tensor([[ 1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
      "          1., -1.],\n",
      "        [-1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
      "          1., -1.],\n",
      "        [ 1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
      "         -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
      "          1., -1.],\n",
      "        [-1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "         -1., -1.],\n",
      "        [ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
      "         -1., -1.],\n",
      "        [ 1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
      "         -1., -1.],\n",
      "        [ 1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
      "         -1., -1.],\n",
      "        [-1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1.,\n",
      "         -1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
      "         -1., -1.],\n",
      "        [ 1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
      "         -1., -1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
      "         -1., -1.],\n",
      "        [ 1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
      "          1.,  1.],\n",
      "        [-1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "         -1.,  1.],\n",
      "        [-1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
      "         -1., -1.],\n",
      "        [-1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
      "          1., -1.]], grad_fn=<SignBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8729,  1.8419, -4.7915, -1.7068, -4.7331,  0.8427,  3.0279, -3.6637,\n",
       "          2.5646,  6.4944,  5.7991,  1.8301,  0.7808, -5.5449,  5.2603,  2.9647]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_layer = BitLinear(16, 16, bit=8)\n",
    "test_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m, \n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_csv_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         }\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/Work/Fall 2023/Projects/Bit_Transformer/BitNet-Transformer/train.py:190\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    187\u001b[0m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_folder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# get dataloaders\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt \u001b[38;5;241m=\u001b[39m \u001b[43mget_datset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# get model\u001b[39;00m\n\u001b[1;32m    193\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(config, tokenizer_src\u001b[38;5;241m.\u001b[39mget_vocab_size(), tokenizer_tgt\u001b[38;5;241m.\u001b[39mget_vocab_size())\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/Work/Fall 2023/Projects/Bit_Transformer/BitNet-Transformer/train.py:142\u001b[0m, in \u001b[0;36mget_datset\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_datset\u001b[39m(config):\n\u001b[1;32m    140\u001b[0m     raw_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang_src\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang_tgt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m     tokenizer_src \u001b[38;5;241m=\u001b[39m \u001b[43mget_or_build_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang_src\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     tokenizer_tgt \u001b[38;5;241m=\u001b[39m get_or_build_tokenizer(raw_dataset, config, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang_tgt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# split into training and validation sets\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/Work/Fall 2023/Projects/Bit_Transformer/BitNet-Transformer/train.py:132\u001b[0m, in \u001b[0;36mget_or_build_tokenizer\u001b[0;34m(dataset, config, language)\u001b[0m\n\u001b[1;32m    130\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m Whitespace() \u001b[38;5;66;03m# split by whitespace\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m WordLevelTrainer(special_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[EOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m], min_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# words must appear at least twice in order to be added to the vocabulary\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mstr\u001b[39m(tokenizer_path))\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model_bitnet import build_bitnet_transformer\n",
    "from train import train_model\n",
    "\n",
    "config = { \"batch_size\": 8, \n",
    "        \"num_epochs\":20,\n",
    "        \"lr\": 0.0001,\n",
    "        \"seq_len\": 500,\n",
    "        \"d_model\": 512,\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"fr\", # change to german later\n",
    "        \"train_data_ratio\": 0.9,\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_filename\": \"transfomer_model_\", \n",
    "        \"preload\": None,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/transformer_model\",\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"transformer_type\": \"bitnet\",\n",
    "        \"loss_csv_file\": \"train_loss.csv\"\n",
    "        }\n",
    "\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
